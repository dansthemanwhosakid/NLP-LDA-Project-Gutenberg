{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Text Mining\n",
    "\n",
    "In natural language processing (NLP), Latent Dirichlet Allocation (LDA) is a popular tool for topic modeling which allows for discovery of patterns in similar topics and semantic structures in a corpus. \n",
    "\n",
    "This notebook contains the cleaning and EDA process.\n",
    "\n",
    "#### Cleaning\n",
    "\n",
    "In the previous notebook, we did some preliminary text cleaning. We will now generate an LDA model for each of the top 20 most prolific writers in the [University of Michigan Gutenberg Dataset](https://web.eecs.umich.edu/~lahiri/gutenberg_dataset.html) and their works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "result_df = pd.read_csv('./data/result_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stop_words.py` contains one function: `custom_stopwords()` that returns a combined list of stopwords from gensim, nltk, spacy, and other sources. **NOTE:** we can continue to add to this stop words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom stopwords list\n",
    "from stop_words import custom_stopwords\n",
    "\n",
    "stop_words = custom_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize Text\n",
    "def regex_tokenizer(text):\n",
    "    text = re.sub('[^\\w\\s]', '', text) # replaces all non letter digit and white space characters\n",
    "    words = re.compile(r'[a-zA-Z0-9]+').findall(text) # tokenizes words\n",
    "    \n",
    "    # remove stop words:\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 Authors & their LDA topics\n",
    "\n",
    "I referenced the following [source](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#11createthedictionaryandcorpusneededfortopicmodeling) to generate the id2word and corpus for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['tokenized'] = result_df['text'].map(lambda x: regex_tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA_htmls folder used to not exist in the data folder. Now it does. Congrats!!!\n"
     ]
    }
   ],
   "source": [
    "# If the folder that stores the author's ldas doesn't exist, create it\n",
    "if os.path.isdir('./data/LDA_htmls') == False:\n",
    "    print(\"LDA_htmls folder used to not exist in the data folder. Now it does. Congrats!!!\")\n",
    "    os.mkdir('./data/LDA_htmls')\n",
    "else:\n",
    "    print(\"Folder LDA_htmls already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the pyLDAvis visualization:\n",
    "\n",
    "The Intertopic Distance Map projects topic clusters as circles. \n",
    "\n",
    "We are using a default PCA model to extract first 2 components (PC1 and PC2) on a topic-term distribution distance matrix. The distance between circles indicates how similar a topic is to one another. The area of the circles is proportional to the proportions of the topics across the total number of tokens in the corpus.\n",
    "\n",
    "The Top 30 Most Salient Terms represent how relevant and salient terms are for a selected topic.\n",
    "1. The red bars represent the most relevant terms for a selected topic. Relevance rank terms within topics for topic interpretation. The way relevance is defined is by the following [source](https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf): \n",
    "\n",
    "$$r(w, t | \\lambda) = \\lambda \\cdot P(w|t) + (1 - \\lambda) \\cdot \\frac{P(w|t)}{P(w)}$$\n",
    "\n",
    "Terms are ranked based on how relevant they are to a given topic and the proportion of their frequency in the topic compared to the rest of the corpus\n",
    "\n",
    "2. The blue bars represent the most salient terms for a selected topic. Saliency is defined as how distinctive a term is for a selected topic. For example, we observe the likelihood that an observed word was generated by a topic and the likelihood that any  [source](http://vis.stanford.edu/files/2012-Termite-AVI.pdf)\n",
    "\n",
    "$$saliency(w) = frequency(w) \\cdot \\sum_t P(t|w) \\cdot \\log \\frac{P(t|w)}{P(t)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "William Dean Howells's works have a total of 46756 words\n",
      "George Alfred Henty's works have a total of 45601 words\n",
      "Edward Stratemeyer's works have a total of 25351 words\n",
      "William Wymark Jacobs's works have a total of 17700 words\n",
      "Henry Rider Haggard's works have a total of 37008 words\n",
      "Sir Arthur Conan Doyle's works have a total of 43490 words\n",
      "Henry James's works have a total of 41678 words\n",
      "Bret Harte's works have a total of 39673 words\n",
      "Nathaniel Hawthorne's works have a total of 33071 words\n",
      "Jacob Abbott's works have a total of 25070 words\n",
      "Edward Phillips Oppenheim's works have a total of 32134 words\n",
      "Anthony Trollope's works have a total of 45842 words\n",
      "Andrew Lang's works have a total of 61704 words\n",
      "Robert Louis Stevenson's works have a total of 50476 words\n",
      "Charles Dickens's works have a total of 46140 words\n",
      "Charlotte Mary Yonge's works have a total of 53964 words\n",
      "Jack London's works have a total of 43013 words\n",
      "Herbert George Wells's works have a total of 50380 words\n",
      "R M Ballantyne's works have a total of 44400 words\n",
      "Mark Twain's works have a total of 45746 words\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "    \n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# Top 20 Authors\n",
    "authors = result_df['author'].unique()\n",
    "\n",
    "for author in authors:\n",
    "    \n",
    "    # Convert author name to lower and replace spaces\n",
    "    file_name = author.lower().replace(' ','_')\n",
    "    \n",
    "    # Grab author's tokenized text data\n",
    "    text_data = result_df[result_df['author'] == author]['tokenized'].tolist()\n",
    "\n",
    "    # Create Dictionary:\n",
    "    id2word = corpora.Dictionary(text_data)\n",
    "\n",
    "    # Print total words for author:\n",
    "    print(f\"{author}'s works have a total of {len(id2word)} words\")\n",
    "\n",
    "    # Convert document into bag of words format\n",
    "    corpus = [id2word.doc2bow(text) for text in text_data]\n",
    "\n",
    "    # LDA model\n",
    "    LDA = gensim.models.ldamodel.LdaModel(corpus = corpus,\n",
    "                                          id2word = id2word,\n",
    "                                          num_topics = 10, # Choosing 10 topics\n",
    "                                          random_state=42,\n",
    "                                          update_every = 2, # online iterative learning instead of batch\n",
    "                                          chunksize = 100,\n",
    "                                          passes = 5,\n",
    "                                          alpha = 'auto',\n",
    "                                          per_word_topics = True)\n",
    "\n",
    "    # Generate top 10 topics\n",
    "    lda_topics = LDA.print_topics()\n",
    "    \n",
    "    # Save pyLDAvis visualization as an html file:\n",
    "    viz = pyLDAvis.gensim.prepare(LDA, corpus, dictionary = LDA.id2word)\n",
    "    pyLDAvis.save_html(viz, f\"./data/LDA_htmls/{file_name}_LDA.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
